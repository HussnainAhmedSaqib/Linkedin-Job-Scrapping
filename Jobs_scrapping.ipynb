{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classified-bennett",
   "metadata": {},
   "source": [
    "# Web Scrapping Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-birmingham",
   "metadata": {},
   "source": [
    "## Install Selenium if you haven't "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "rolled-exhibit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from selenium) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-evolution",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "challenging-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.expected_conditions import staleness_of\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-serum",
   "metadata": {},
   "source": [
    "## Enter Linkedin Job URL from where you wish to scrap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "extraordinary-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.linkedin.com/jobs/search/?keywords=data%20scientist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-burden",
   "metadata": {},
   "source": [
    "## Download Chromedriver (different from chrome web browser) and place in your directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "southern-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = webdriver.Chrome(executable_path=\"C:\\Program Files (x86)/chromedriver.exe\") \n",
    "wd.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-accident",
   "metadata": {},
   "source": [
    "## Enter no of jobs you want to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "structured-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_jobs = 20#int(wd.find_element_by_css_selector('h1>span').get_attribute('innerText'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-ballet",
   "metadata": {},
   "source": [
    "## Scroll Down and load jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "accessory-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "while i<= int(no_of_jobs)/25+1:\n",
    "    wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    i+=1\n",
    "    try:\n",
    "        wd.find_element_by_xpath('/html/body/main/div/section/button').click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-tomato",
   "metadata": {},
   "source": [
    "## Put the number of jobs on the page in a list variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "united-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_lists = wd.find_element_by_class_name('jobs-search__results-list')\n",
    "jobs = job_lists.find_elements_by_tag_name('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "institutional-recognition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-assumption",
   "metadata": {},
   "source": [
    "## Initialize Variables in which you'll be storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "standard-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id= []\n",
    "job_title = []\n",
    "company_name = []\n",
    "location = []\n",
    "jd=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-advisory",
   "metadata": {},
   "source": [
    "## Make a function to assign unique job ids to each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "sought-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import string \n",
    "def WithoutRepeat(length):  \n",
    "    letters = string.ascii_lowercase # define the specific string  \n",
    "    # define the condition for random.sample() method  \n",
    "    result1 = ''.join((random.sample(letters, length)))   \n",
    "    return result1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-borough",
   "metadata": {},
   "source": [
    "## Append Job ID, Job Title, Company Name and Location to a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "limiting-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs:\n",
    "    job_id.append(WithoutRepeat(8))\n",
    "    \n",
    "    job_title0 = job.find_element_by_class_name('base-search-card__title').get_attribute('innerText')\n",
    "    job_title.append(job_title0)\n",
    "    \n",
    "    company_name0 = job.find_element_by_class_name('base-search-card__subtitle').get_attribute('innerText')\n",
    "    company_name.append(company_name0)\n",
    "    \n",
    "    location0 = job.find_element_by_class_name('job-search-card__location').get_attribute('innerText')\n",
    "    location.append(location0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-viking",
   "metadata": {},
   "source": [
    "## Scrap Job Descriptions separately (this is a little tricky, this is why we are doing it separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "adaptive-accordance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "#get job descriptions\n",
    "jd=[]\n",
    "for item in range(len(jobs)):\n",
    "    # clicking job to view job details\n",
    "    job_click_path = '/html/body/div[1]/div/main/section[2]/ul/li[' + str(item+1) + ']/div/a'  \n",
    "    job_click = job.find_element_by_xpath(job_click_path).click()\n",
    "    time.sleep(5)\n",
    "    jd_path = '/html/body/div[1]/div/section/div[2]/section[2]/div/section/div' \n",
    "    try:\n",
    "        jd0 = job.find_element_by_xpath(jd_path).get_attribute('innerText')\n",
    "        jd.append(jd0)\n",
    "    except:\n",
    "        jd.append('NO JOB DESCRIPTION')\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-hygiene",
   "metadata": {},
   "source": [
    "## Make a Dictionary in which you can store the lists containing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "large-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details={'jobID':job_id,'job_title':job_title,'company_name':company_name,'location':location,'job_descrption':jd}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-offering",
   "metadata": {},
   "source": [
    "## Convert Dictionary to a Data Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "obvious-latitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobID</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>job_descrption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iyocwtal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Stash</td>\n",
       "      <td>United States</td>\n",
       "      <td>Want to help everyday Americans invest and bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tdclkona</td>\n",
       "      <td>Data &amp; Applied Scientist</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Microsoft’s mission is to empower every person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fdvhcsrk</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Chewy</td>\n",
       "      <td>Dania, FL</td>\n",
       "      <td>Our Opportunity\\n\\nChewy is looking for an exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jqcvkulr</td>\n",
       "      <td>Entry Level - Associate Data Scientist (Federal)</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Hampton, VA</td>\n",
       "      <td>Introduction\\n\\nAs an Associate Data Scientist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>etqsjvbm</td>\n",
       "      <td>Data Scientist (Remote)</td>\n",
       "      <td>SkillSoniq</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Position: Healthcare Business Analysts analyze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mcalwxdh</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>NO JOB DESCRIPTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lcvmyzsr</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Visa</td>\n",
       "      <td>Foster City, CA</td>\n",
       "      <td>NO JOB DESCRIPTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dvhkuqbj</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>O'Reilly Auto Parts</td>\n",
       "      <td>United States</td>\n",
       "      <td>NO JOB DESCRIPTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qxmusfao</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>NO JOB DESCRIPTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hkgensmx</td>\n",
       "      <td>Data Scientist - Shop Recommendations</td>\n",
       "      <td>Stitch Fix</td>\n",
       "      <td>United States</td>\n",
       "      <td>About The Team\\n\\nAt Stitch Fix, our data scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jxtbengl</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Stash</td>\n",
       "      <td>United States</td>\n",
       "      <td>Want to help everyday Americans invest and bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hcsgmdqi</td>\n",
       "      <td>Data Scientist I- 100% Remote Work</td>\n",
       "      <td>USAA</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>Purpose of Job\\n\\nWe are seeking a talented Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mvcbjtuf</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>DeepIntent</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>What We Are Looking For:\\n\\nWe are growing the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pqxyarsz</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Introduction\\n\\nAs a Data Scientist at IBM, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hupelmxr</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>Portland, OR</td>\n",
       "      <td>Our Story\\n\\nXandr is a technology platform po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ahgnxyqz</td>\n",
       "      <td>Data Scientist-REMOTE</td>\n",
       "      <td>Jobot</td>\n",
       "      <td>Portland, OR</td>\n",
       "      <td>This AI Computer Software company located in P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zswobjnc</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Cloudflare</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>\\n        This AI Computer Software company lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ychikdaz</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>\\n        This AI Computer Software company lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fibsghyu</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "      <td>About Us\\n\\nAt Cloudflare, we have our eyes se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vujfkipb</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Uptake</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>What we do:\\n\\nUptake is the premier Industria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pvfqeydj</td>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>In this age of disruption, organizations need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ktmbpsqv</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Raleigh, NC</td>\n",
       "      <td>Introduction\\n\\nAs a Data Scientist at IBM, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ykgbtejo</td>\n",
       "      <td>Data &amp; Applied Scientist</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Are you looking for an opportunity to solve bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>zxhfyslu</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Children's Hospital Los Angeles (CHLA)</td>\n",
       "      <td>Glendale, CA</td>\n",
       "      <td>NO JOB DESCRIPTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ligowjyt</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Pagaya</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>About Pagaya\\n\\nPagaya is a financial technolo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       jobID                                         job_title  \\\n",
       "0   iyocwtal                                    Data Scientist   \n",
       "1   tdclkona                          Data & Applied Scientist   \n",
       "2   fdvhcsrk                                    Data Scientist   \n",
       "3   jqcvkulr  Entry Level - Associate Data Scientist (Federal)   \n",
       "4   etqsjvbm                           Data Scientist (Remote)   \n",
       "5   mcalwxdh                                    Data Scientist   \n",
       "6   lcvmyzsr                                    Data Scientist   \n",
       "7   dvhkuqbj                                    Data Scientist   \n",
       "8   qxmusfao                                    Data Scientist   \n",
       "9   hkgensmx             Data Scientist - Shop Recommendations   \n",
       "10  jxtbengl                                    Data Scientist   \n",
       "11  hcsgmdqi                Data Scientist I- 100% Remote Work   \n",
       "12  mvcbjtuf                                    Data Scientist   \n",
       "13  pqxyarsz                                    Data Scientist   \n",
       "14  hupelmxr                                    Data Scientist   \n",
       "15  ahgnxyqz                             Data Scientist-REMOTE   \n",
       "16  zswobjnc                                    Data Scientist   \n",
       "17  ychikdaz                                    Data Scientist   \n",
       "18  fibsghyu                                    Data Scientist   \n",
       "19  vujfkipb                                    Data Scientist   \n",
       "20  pvfqeydj                             Junior Data Scientist   \n",
       "21  ktmbpsqv                                    Data Scientist   \n",
       "22  ykgbtejo                          Data & Applied Scientist   \n",
       "23  zxhfyslu                                    Data Scientist   \n",
       "24  ligowjyt                                    Data Scientist   \n",
       "\n",
       "                              company_name         location  \\\n",
       "0                                    Stash    United States   \n",
       "1                                Microsoft       Irving, TX   \n",
       "2                                    Chewy        Dania, FL   \n",
       "3                                      IBM      Hampton, VA   \n",
       "4                               SkillSoniq  Los Angeles, CA   \n",
       "5                                  Walmart       Dallas, TX   \n",
       "6                                     Visa  Foster City, CA   \n",
       "7                      O'Reilly Auto Parts    United States   \n",
       "8                                   Amazon      Seattle, WA   \n",
       "9                               Stitch Fix    United States   \n",
       "10                                   Stash    United States   \n",
       "11                                    USAA  San Antonio, TX   \n",
       "12                              DeepIntent     New York, NY   \n",
       "13                                     IBM       Austin, TX   \n",
       "14                                    AT&T     Portland, OR   \n",
       "15                                   Jobot     Portland, OR   \n",
       "16                              Cloudflare       Austin, TX   \n",
       "17                                Deloitte   Washington, DC   \n",
       "18                                     IBM      Raleigh, NC   \n",
       "19                                  Uptake      Chicago, IL   \n",
       "20                                Deloitte    Arlington, VA   \n",
       "21                                     IBM      Raleigh, NC   \n",
       "22                               Microsoft     Bellevue, WA   \n",
       "23  Children's Hospital Los Angeles (CHLA)     Glendale, CA   \n",
       "24                                  Pagaya       Boston, MA   \n",
       "\n",
       "                                       job_descrption  \n",
       "0   Want to help everyday Americans invest and bui...  \n",
       "1   Microsoft’s mission is to empower every person...  \n",
       "2   Our Opportunity\\n\\nChewy is looking for an exp...  \n",
       "3   Introduction\\n\\nAs an Associate Data Scientist...  \n",
       "4   Position: Healthcare Business Analysts analyze...  \n",
       "5                                  NO JOB DESCRIPTION  \n",
       "6                                  NO JOB DESCRIPTION  \n",
       "7                                  NO JOB DESCRIPTION  \n",
       "8                                  NO JOB DESCRIPTION  \n",
       "9   About The Team\\n\\nAt Stitch Fix, our data scie...  \n",
       "10  Want to help everyday Americans invest and bui...  \n",
       "11  Purpose of Job\\n\\nWe are seeking a talented Da...  \n",
       "12  What We Are Looking For:\\n\\nWe are growing the...  \n",
       "13  Introduction\\n\\nAs a Data Scientist at IBM, yo...  \n",
       "14  Our Story\\n\\nXandr is a technology platform po...  \n",
       "15  This AI Computer Software company located in P...  \n",
       "16  \\n        This AI Computer Software company lo...  \n",
       "17  \\n        This AI Computer Software company lo...  \n",
       "18  About Us\\n\\nAt Cloudflare, we have our eyes se...  \n",
       "19  What we do:\\n\\nUptake is the premier Industria...  \n",
       "20  In this age of disruption, organizations need ...  \n",
       "21  Introduction\\n\\nAs a Data Scientist at IBM, yo...  \n",
       "22  Are you looking for an opportunity to solve bu...  \n",
       "23                                 NO JOB DESCRIPTION  \n",
       "24  About Pagaya\\n\\nPagaya is a financial technolo...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(job_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-declaration",
   "metadata": {},
   "source": [
    "## Save the Data Frame as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "personal-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(job_details).to_csv('linkedin_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
